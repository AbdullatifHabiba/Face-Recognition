{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullatifHabiba/Face-Recognition/blob/main/LDA_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5UotrXNgXET"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from matplotlib.image import imread\n",
        "from zipfile import ZipFile\n",
        "from scipy.linalg import eigh\n",
        "\n",
        "#with ZipFile(\"archive.zip\", \"r\") as folders:\n",
        "# folders.extractall(\"data\")\n",
        "with ZipFile(\"nonface.zip\", \"r\") as fol:\n",
        "  fol.extractall(\"noneFace\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQSzISpxeunV",
        "outputId": "874a6cfe-497e-42e7-8d9e-fdd76837893d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-nnTZBOiB64"
      },
      "source": [
        "#Creating the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jZ90Fhpha_t"
      },
      "source": [
        "Here we read images bytes and store them in dataMatrix with size 400*10304 and create labelVector that holds the labels of the dataMatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVg_9kV-gxtE"
      },
      "outputs": [],
      "source": [
        "dataMatrix = []\n",
        "labelVector = []\n",
        "for i in range(1, 41):\n",
        "  for j in range(1, 11):\n",
        "    image = imread(\"data/s\" + str(i) + \"/\" + str(j) + \".pgm\")\n",
        "    labelVector.append(i);\n",
        "    dataMatrix.append(image.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfLGKdsqiNnl"
      },
      "source": [
        "Here we construct the training data and testing data with their labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEc4ovLzPorQ"
      },
      "outputs": [],
      "source": [
        "def devide_data(data, ratio):\n",
        "  dataTraining = []\n",
        "  dataTesting = []\n",
        "  trainLabel = []\n",
        "  testLabel = []\n",
        "  for i in range(len(data)):\n",
        "    if(int((i + 1) * ratio) != int(i * ratio)):\n",
        "      dataTraining.append(data[i])\n",
        "      trainLabel.append(labelVector[i])\n",
        "    else:\n",
        "      dataTesting.append(data[i])\n",
        "      testLabel.append(labelVector[i])\n",
        "  return dataTraining, dataTesting, trainLabel, testLabel\n",
        "dataTraining, dataTesting, trainLabel, testLabel = devide_data(dataMatrix, .5)\n",
        "TestDFrame = pd.DataFrame(data = dataTesting)\n",
        "TrainDFrame = pd.DataFrame(data = dataTraining)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYS1VGiJ5qDI"
      },
      "source": [
        "#PCA Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp7Q9qTZ0SUt"
      },
      "outputs": [],
      "source": [
        "def calculate_eigenValues_eigenVectors(data):\n",
        "  centered_data = data.apply(lambda x: x-x.mean())\n",
        "  cov_matrix = np.cov(centered_data.T, bias = True)\n",
        "  eignValue, eignVector = eigh(cov_matrix)\n",
        "  return eignValue, eignVector\n",
        "eignValue, eignVector = calculate_eigenValues_eigenVectors(TrainDFrame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QihAdfe4dPa"
      },
      "outputs": [],
      "source": [
        "def PCA(alpha, data, eignValue, eignVector):\n",
        "  for r in range(1, len(eignValue)):\n",
        "    if sum(eignValue[-r:]) / sum(eignValue) >= alpha:\n",
        "      break;\n",
        "  return eignVector[-r:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdlb-Th5jArO"
      },
      "source": [
        "K_NN algorithm in PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjmQEaQ2Fjl2"
      },
      "outputs": [],
      "source": [
        "# simple classifier (first Nearest Neighbor to determine the class labels\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "def classify_by_KNN(K, trained_data, tested_data):\n",
        "  Fnn = KNeighborsClassifier(n_neighbors = K)\n",
        "  Fnn.fit(trained_data, trainLabel)\n",
        "  tested_result = Fnn.predict(tested_data)\n",
        "  ac_dif = np.array(tested_result - trainLabel[:len(tested_result)])\n",
        "  # print(\"train accuracy\", Fnn.score(trained_data, trainLabel))\n",
        "  # print(\"test accuracy\", Fnn.score(tested_data, testLabel))\n",
        "  return Fnn.score(trained_data, trainLabel), Fnn.score(tested_data, testLabel)\n",
        "for alpha in [.8, .85, .9, .95]:\n",
        "  Projection_matrix = PCA(alpha, TrainDFrame, eignValue, eignVector)\n",
        "  TrainingAfter_projection = np.dot(Projection_matrix, np.transpose(TrainDFrame))\n",
        "  TestingAfter_projection = np.dot(Projection_matrix,np.transpose(TestDFrame))\n",
        "  print(TestingAfter_projection.shape)\n",
        "  tested_data = np.transpose(TestingAfter_projection)\n",
        "  trained_data = TrainingAfter_projection.T\n",
        "  # k 1 3 5 7\n",
        "  K_arr = [1, 3, 5, 7]\n",
        "  matrix = np.zeros((4, 2))\n",
        "  i = 0\n",
        "  for x in K_arr:\n",
        "    matrix[i] = classify_by_KNN(x, trained_data, tested_data)\n",
        "    i = i + 1\n",
        "  plt.plot(K_arr, matrix[:, 1:], label = 'Testing dataset Accuracy')\n",
        "  plt.plot(K_arr, matrix[:, 0:1], label = 'Training dataset Accuracy')\n",
        "  plt.legend()\n",
        "  plt.title('alpha = ' + str(alpha))\n",
        "  plt.xlabel('k')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1RPuWjcxFR1"
      },
      "source": [
        "# LDA Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwa4f0K19v2M"
      },
      "outputs": [],
      "source": [
        "def LDA(dataT, size, step, trainLabel):\n",
        "  means = np.zeros(((int) (size / step), len(dataT[0])))\n",
        "  i = 0\n",
        "  for v in range(0, size, step):\n",
        "    means[i] = np.mean(dataT[v:v + step], axis = 0);\n",
        "    i = i + 1\n",
        "  # calculate of  ùëÜùëè by ‚àëùëõùëò(ùúáùëò ‚àí ùúá)(ùúáùëò ‚àí ùúá)\n",
        "  # calculate over all mean\n",
        "  over_all_mean = np.mean(means, axis = 0)\n",
        "  SB = np.zeros((len(means[0]), len(means[0])))\n",
        "  for m in range(len(means)):\n",
        "    SB = np.add(SB, step*((means[m] - over_all_mean) * (means[m] - over_all_mean).T))\n",
        "  #iii.  CENTER data\n",
        "  center_data = []\n",
        "  if step !=5:\n",
        "        center_data[0:step] = np.subtract(dataT[0:step], means[0])\n",
        "        center_data[step:size+1] = np.subtract(dataT[step:size+1], means[1])\n",
        "  else:\n",
        "     for i in range(len(dataT)):\n",
        "       c = np.array(dataT)[i,:] - np.array(means)[(trainLabel[i] - 1),:];\n",
        "       center_data.insert(i, c)\n",
        "  # cal S = ZT.Z\n",
        "  S = np.dot(np.array(center_data).T, np.array(center_data))\n",
        "  S_inv = np.linalg.inv(S)\n",
        "  #cal eigen value and eigen vector\n",
        "  A = np.dot(S_inv,SB)\n",
        "  eigen_value, eigenvector = eigh(A, subset_by_index = [10304 - 40, 10304 - 1])\n",
        "  return eigenvector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIA3QJ_IWBfI"
      },
      "outputs": [],
      "source": [
        "# trained data\n",
        "eigenvector = LDA(dataTraining, 200, 5, trainLabel)\n",
        "trained_data = np.dot(dataTraining, eigenvector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VjFGNxA7LL_"
      },
      "outputs": [],
      "source": [
        "# tested data\n",
        "tested_data = np.dot(dataTesting, eigenvector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN-pqmsijH0-"
      },
      "source": [
        "K_NN Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhA7-m5m8E7Z"
      },
      "outputs": [],
      "source": [
        "# simple classifier (first Nearest Neighbor to determine the class labels\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "def classify_by_KNN(K, trained_data, trainLabel, tested_data, testLabel):\n",
        "  Fnn = KNeighborsClassifier(n_neighbors = K)\n",
        "  Fnn.fit(trained_data, trainLabel)\n",
        "  tested_result = Fnn.predict(tested_data)\n",
        "  ac_dif = np.array(tested_result - trainLabel[:len(tested_result)])\n",
        "  return Fnn.score(trained_data, trainLabel), Fnn.score(tested_data, testLabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5GR9pwLH8aH"
      },
      "outputs": [],
      "source": [
        "# k 1 3 5 7\n",
        "K_arr = [1, 3, 5, 7]\n",
        "matrix = np.zeros((4, 2))\n",
        "i = 0\n",
        "for x in K_arr:\n",
        "  matrix[i] = classify_by_KNN(x,trained_data,trainLabel,tested_data,testLabel)\n",
        "  i = i + 1\n",
        "plt.plot(K_arr, matrix[:, 1:], label = 'Testing dataset Accuracy')\n",
        "plt.plot(K_arr, matrix[:, 0:1], label = 'Training dataset Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CbT7cT1IL0N"
      },
      "source": [
        "\n",
        "\n",
        "## **Compare vs Non-Face Images**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEMpJc4mLCui"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "NonFaceMatrix = [];\n",
        "for i in range(150):\n",
        "  image = cv2.imread(\"drive/MyDrive/natural_images/airplane/airplane_\" + str(i).zfill(4) + \".jpg\")\n",
        "  image\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  res = cv2.resize(image, dsize=(92, 112), interpolation=cv2.INTER_CUBIC)\n",
        "  NonFaceMatrix.append(res.flatten())\n",
        "  image = cv2.imread(\"drive/MyDrive/natural_images/car/car_\" + str(i).zfill(4) + \".jpg\")\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  res = cv2.resize(image, dsize=(92, 112), interpolation=cv2.INTER_CUBIC)\n",
        "  NonFaceMatrix.append(res.flatten())\n",
        "  image = cv2.imread(\"drive/MyDrive/natural_images/cat/cat_\" + str(i).zfill(4) + \".jpg\")\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  res = cv2.resize(image, dsize=(92, 112), interpolation=cv2.INTER_CUBIC)\n",
        "  NonFaceMatrix.append(res.flatten())\n",
        "  image = cv2.imread(\"drive/MyDrive/natural_images/dog/dog_\" + str(i).zfill(4) + \".jpg\")\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  res = cv2.resize(image, dsize=(92, 112), interpolation=cv2.INTER_CUBIC)\n",
        "  NonFaceMatrix.append(res.flatten())\n",
        "  image = cv2.imread(\"drive/MyDrive/natural_images/flower/flower_\" + str(i).zfill(4) + \".jpg\")\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  res = cv2.resize(image, dsize=(92, 112), interpolation=cv2.INTER_CUBIC)\n",
        "  NonFaceMatrix.append(res.flatten())\n",
        "  image = cv2.imread(\"drive/MyDrive/natural_images/fruit/fruit_\" + str(i).zfill(4) + \".jpg\")\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  res = cv2.resize(image, dsize=(92, 112), interpolation=cv2.INTER_CUBIC)\n",
        "  NonFaceMatrix.append(res.flatten())\n",
        "  image = cv2.imread(\"drive/MyDrive/natural_images/motorbike/motorbike_\" + str(i).zfill(4) + \".jpg\")\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  res = cv2.resize(image, dsize=(92, 112), interpolation=cv2.INTER_CUBIC)\n",
        "  NonFaceMatrix.append(res.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gExWuYPqnSjQ"
      },
      "outputs": [],
      "source": [
        "# apply lda algrothim for this data  400 image [200 non face,200 face]\n",
        "def nonFace_compare(data_size,nonface_size):\n",
        "  d=(int)(nonface_size/2)\n",
        "  NonFaceCompareM_train = np.array(dataTraining + NonFaceMatrix[0:d])\n",
        "  NonFaceCompareL_train = np.concatenate([np.zeros(200), np.ones(d)])\n",
        "  print(NonFaceCompareM_train.shape)\n",
        "  eigv = LDA(NonFaceCompareM_train, data_size, d, NonFaceCompareL_train)\n",
        "  newDataCompare = np.dot(NonFaceCompareM_train,eigv)\n",
        "  NonFaceCompareM_test = np.array(dataTesting+NonFaceMatrix[d:nonface_size])\n",
        "  NonFaceCompareL2_test = np.concatenate([np.zeros(200), np.ones(d)])\n",
        "  newTestCompare = np.dot(NonFaceCompareM_test, eigv)\n",
        "  return  newDataCompare,NonFaceCompareL_train,newTestCompare,NonFaceCompareL2_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfRrdIAiSX4h"
      },
      "outputs": [],
      "source": [
        "\n",
        "K_arr = [1, 3, 5, 7]\n",
        "nonface_images=[400,600,800,1000]\n",
        "matrix1 = np.zeros((4, 2))\n",
        "res=[]\n",
        "\n",
        "for j in nonface_images:\n",
        "  i = 0\n",
        "  for x in K_arr:\n",
        "    matrix1[i] = classify_by_KNN(x,nonFace_compare((int)((400+j)/2),j))\n",
        "    i = i + 1\n",
        "\n",
        "  plt.plot(K_arr, matrix1[:, 1:], label = 'Testing dataset Accuracy')\n",
        "  plt.plot(K_arr, matrix1[:, 0:1], label = 'Training dataset Accuracy')\n",
        "  plt.legend()\n",
        "  plt.xlabel('k')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.show()\n",
        "  res.append(matrix1)\n",
        "res\n",
        "plt.scatter(nonface_images,np.mean(matrix1,axis=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMZWPas1Fike"
      },
      "source": [
        "#Second bonus part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdYtkZFS5I-E"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "X = (dataTraining);\n",
        "no_of_batches = 1;\n",
        "incremental_pca = IncrementalPCA(n_components = 170)\n",
        "for batch in np.array_split(X, no_of_batches):\n",
        "  incremental_pca.fit(batch)\n",
        "finalTraining = incremental_pca.transform(X)\n",
        "print(finalTraining.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa8PBXIxEg3S"
      },
      "outputs": [],
      "source": [
        "X = (dataTesting);\n",
        "no_of_batches = 1;\n",
        "incremental_pca = IncrementalPCA(n_components = 170)\n",
        "for batch in np.array_split(X, no_of_batches):\n",
        "  incremental_pca.fit(batch)\n",
        "finalTesting = incremental_pca.transform(X)\n",
        "print(finalTesting.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fty2vldM_RXf"
      },
      "outputs": [],
      "source": [
        "tested_data = finalTesting\n",
        "trained_data = finalTraining\n",
        "# k 1 3 5 7\n",
        "K_arr = [1, 3, 5, 7]\n",
        "matrix = np.zeros((4, 2))\n",
        "i = 0\n",
        "for x in K_arr:\n",
        "  matrix[i] = classify_by_KNN(x, trained_data, trainLabel, tested_data, testLabel)\n",
        "  i = i + 1\n",
        "print(matrix)\n",
        "plt.plot(K_arr, matrix[:, 1:], label = 'Testing dataset Accuracy')\n",
        "plt.plot(K_arr, matrix[:, 0:1], label = 'Training dataset Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGOc9xOTVkSk"
      },
      "source": [
        "#Shrinkage linear discriminant analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB9KylUxVgxe"
      },
      "source": [
        "Shrinkage linear discriminant analysis is a variant of normal one but uses shrinkage regulator to estimate the covariance matrix, to increase stability and solve overfitting problems in normal LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YOsLP7QOJ0t"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = dataMatrix\n",
        "y = labelVector\n",
        "X_train, X_test,\\\n",
        "    y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
        "estimator = LinearDiscriminantAnalysis(solver = 'eigen', shrinkage = 'auto')\n",
        "estimator.fit(X_train, y_train)\n",
        "y_pred = estimator.predict(X_test)\n",
        "print(estimator.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kvevg3PsSnS5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}